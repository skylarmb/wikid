Metadata-Version: 2.4
Name: qwen-server
Version: 0.1.0
Summary: Simple vLLM server for Qwen3-8B-FP8 model
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: vllm>=0.10.0
Requires-Dist: openai<=1.90.0,>=1.87.0
Requires-Dist: fastapi
Requires-Dist: uvicorn

# Qwen3-8B-FP8 vLLM Server

A simple Python package for serving the Qwen3-8B-FP8 model using vLLM with OpenAI-compatible API.

## Installation

```bash
# Install the package in development mode
uv pip install -e .
```

Or sync dependencies from pyproject.toml:

```bash
uv sync
```

## Usage

### Start the Server

```bash
# Using the package entry point
uv run qwen-server

# Or run directly with uv
uv run python -m qwen_server.server

# With custom options
uv run qwen-server --host 0.0.0.0 --port 8080 --api-key your-api-key
```

### Client Example

```bash
# Test with the included client
uv run qwen-client "Hello, how are you?"

# Or run the module directly
uv run python -m qwen_server.client "Hello, how are you?"

# With custom options
uv run qwen-client "What is the capital of France?" --max-tokens 100 --temperature 0.5
```

### Python API

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="your-api-key"  # or "dummy-key" if no auth
)

response = client.chat.completions.create(
    model="qwen-community/Qwen3-8B-FP8",
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)

print(response.choices[0].message.content)
```

## Configuration Options

- `--model`: Model name (default: qwen-community/Qwen3-8B-FP8)
- `--host`: Host to bind to (default: 0.0.0.0)
- `--port`: Port to bind to (default: 8000)
- `--api-key`: API key for authentication
- `--dtype`: Data type for model weights (default: auto)
- `--max-model-len`: Maximum model length
- `--tensor-parallel-size`: Number of GPUs for tensor parallelism (default: 1)

## Requirements

- Python 3.8+
- vLLM 0.6.0+
- CUDA-compatible GPU (recommended)
